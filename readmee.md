# ğŸš€ Automated Kubernetes Cluster on AWS using Terraform + Ansible + kubeadm

![Terraform](https://img.shields.io/badge/Terraform-IaC-623CE4?logo=terraform&logoColor=white)
![Ansible](https://img.shields.io/badge/Ansible-Automation-EE0000?logo=ansible&logoColor=white)
![Kubernetes](https://img.shields.io/badge/Kubernetes-kubeadm-326CE5?logo=kubernetes&logoColor=white)
![AWS](https://img.shields.io/badge/AWS-Cloud-232F3E?logo=amazon-aws&logoColor=white)

This project automates a **production-like Kubernetes cluster on AWS** from scratch using:

âœ” **Terraform** â€“ Infrastructure provisioning (Dev & Prod environments)  
âœ” **Ansible** â€“ Automates installation + kubeadm cluster setup  
âœ” **kubeadm** â€“ Initializes the master and joins worker nodes  
âœ” **Calico CNI** â€“ Installed automatically via Ansible during control plane bootstrap  

---
## âœ… Project Overview

| Component         | Purpose |
|------------------|---------|
| **Terraform**    | Creates VPC, Bastion Host, 1 Master, 2 Workers |
| **Dev & Prod Environments** | Isolated workspaces under `infra/terraform/envs/` |
| **Dynamic Inventory** | Auto-generated `inventory.ini` from Terraform output |
| **Ansible Playbooks** | Installs Docker, containerd, kubeadm, kubelet, CNI |
| **Calico CNI**   | Installed automatically via Ansible in `03-init-control-plane.yaml` |
| **Challenge Faced** | Calico CNI Pods kept crashing due to AWS networking (IPIP/BGP mismatch) |

---

## ğŸ—ï¸ Architecture Diagram

![Architecture of
kubeadm-aws-platform-engineering](/images/diagram-kubeadm-aws-platform-engineering.png)

```
ğŸ“ images/
    |
    â””â”€â”€ diagram-kubeadm-aws-platform-engineering.png
```
## ğŸ“ Repository Structure

```
â”œâ”€â”€ infra/
â”‚   â””â”€â”€ terraform/
â”‚       â”œâ”€â”€ envs/
â”‚       â”‚   â”œâ”€â”€ dev/
â”‚       â”‚   â””â”€â”€ prod/
â”‚       â””â”€â”€ modules/
â”œâ”€â”€ ansible/
â”‚   â”œâ”€â”€ inventory/dev/inventory.ini   # Auto-generated
â”‚   â””â”€â”€ playbooks/
â”‚       â”œâ”€â”€ 01-setup-base.yaml
â”‚       â”œâ”€â”€ 02-install-k8s.yaml
â”‚       â”œâ”€â”€ 03-init-control-plane.yaml   # Installs Calico CNI here
â”‚       â”œâ”€â”€ 04-join-workers.yaml
â”‚       â”œâ”€â”€ 05-verify-cluster.yaml
â”‚       â””â”€â”€ 06-fix-network.yaml
```

## ğŸš€ Deployment Guide
### 1ï¸âƒ£ Provision AWS Infrastructure (Terraform)

```
cd infra/terraform/envs/dev
terraform init                     # Initialize backend and providers
terraform apply -auto-approve      # Create VPC, Bastion, Master, Worker nodes
```

### 2ï¸âƒ£ SSH into Bastion Host

```
ssh-add ~/.ssh/kubeadm-aws-key         # Add SSH key to agent
ssh -A ubuntu@<bastion_public_ip>      # Connect to bastion
```
### 3ï¸âƒ£ Run Ansible â€“ Full Kubernetes Setup

```
ansible-playbook -i inventory/dev/inventory.ini playbooks/01-setup-base.yaml
# Sets hostname, disables swap, updates packages

ansible-playbook -i inventory/dev/inventory.ini playbooks/02-install-k8s.yaml
# Installs containerd, kubelet, kubeadm, kubectl

ansible-playbook -i inventory/dev/inventory.ini playbooks/03-init-control-plane.yaml
# kubeadm init + applies Calico CNI automatically
# Also saves join command â†’ artifacts/kubeadm_join.sh

ansible-playbook -i inventory/dev/inventory.ini playbooks/04-join-workers.yaml
# Uses saved join command to add workers to cluster
```
### 4ï¸âƒ£ Verify Cluster

```
kubectl get nodes        # Master + Worker nodes should show "Ready"
kubectl get pods -A      # Calico, CoreDNS, kube-system pods running
```

## âš  Challenges & Lessons Learned

| Challenge                       | What Happened?                                            |
| ------------------------------- | --------------------------------------------------------- |
| â— Calico CNI Pods kept crashing | Due to AWS VPC routing + IP-in-IP tunneling conflict      |
| â— Manual kubeconfig handling    | Had to copy `/etc/kubernetes/admin.conf` to user manually |
| â— No HA Control Plane           | kubeadm master is a single point of failure               |
| â— Requires SSH Debugging        | No CloudWatch or managed logging like EKS                 |


## âœ… Why AWS EKS is Easier for Production

âœ” AWS manages control plane, etcd, certificates

âœ” VPC CNI works out-of-the-box (no Calico issues)

âœ” IAM Roles for Service Accounts, Cluster Autoscaler support

âœ” Built-in CloudWatch logging, no SSH into nodes

âœ” Letâ€™s focus on CI/CD, security, apps â€” not cluster plumbing

## ğŸ§¹ Cleanup & Destroy Resources 
### âœ… Destroy infrastructure via Terraform:

```
cd infra/terraform/envs/dev
terraform destroy -auto-approve
```
### âœ… Optional manual cleanup:


ğŸ”¹Delete SSH key pair if unused

ğŸ”¹Remove ~/.kube/config from bastion/local machine

ğŸ”¹Release Elastic IPs, delete Route53 records

ğŸ”¹Delete S3 backend & DynamoDB state lock table (if used)

### ğŸ¯ Next Steps

ğŸ”¹ Terraform + AWS EKS (Managed Kubernetes)

ğŸ”¹ Deploy GCP Online Boutique microservices

ğŸ”¹ GitHub Actions + Vault for CI/CD secrets

ğŸ”¹ Karpenter autoscaling + SonarQube + Nexus + ECR

## â­ Like This Project?

If this helped you, feel free to:

  â­ Star the repository

  ğŸ› ï¸ Fork and build on top of it

  ğŸ¤ Connect & collaborate
